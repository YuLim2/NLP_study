{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Ex.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### CNN 모델로 챗봇 문답 데이터 감정 분류 모델 구현"
      ],
      "metadata": {
        "id": "kPFUIbva69-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "erWA4zxOyU7d"
      },
      "outputs": [],
      "source": [
        "# 필요한 모듈 임포트\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 읽어오기\n",
        "train_file = '/content/ChatbotData.csv'\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()"
      ],
      "metadata": {
        "id": "Zl5m7kCz8IcA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pandas의 read_csv()함수를 사용하여 csv 파일 읽기<br>\n",
        "CNN 모델 학습에 필요한 것들을 리스트로 저장\n",
        "- Q(질문)\n",
        "- label(감정)\n"
      ],
      "metadata": {
        "id": "j5GTy4ZYROGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')"
      ],
      "metadata": {
        "id": "XsWitFKO9gHE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "features의 문장들을 sequence() 함수를 이용해 단어 시퀀스 생성<br>\n",
        "생성된 시퀀스를 corpus 리스트에 저장하고 texts_to_sequence() 함수를 이용하여 시퀀스 번호로 변환<br>\n",
        "변환된 시퀀스 번호를 이용하여 단어 임베딩 벡터를 만들고 크기 조정<br>\n",
        "패딩padding 처리"
      ],
      "metadata": {
        "id": "5WUVFuP2R-Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습용, 검증용, 테스트셋 생성 7:2:1\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "\n",
        "train_size = int(len(padded_seqs)*0.7)\n",
        "val_size = int(len(padded_seqs)*0.2)\n",
        "test_size = int(len(padded_seqs)*0.1)\n",
        "\n",
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
        "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)"
      ],
      "metadata": {
        "id": "DrFzXanfA0QP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 랜덤으로 섞은 후 비율에 맞추어 분리"
      ],
      "metadata": {
        "id": "EVsSxfiyWSKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "dropout_prob = 0.5\n",
        "EMB_SIZE = 128\n",
        "EPOCH = 5\n",
        "VOCAB_SIZE = len(word_index) + 1 # 전체 단어 수"
      ],
      "metadata": {
        "id": "C3SeUShhD82g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropout_prob: dropout 생성률 <br>\n",
        "EMB_SIZE: 임베딩 결과로 나올 밀집 벡터의 크기"
      ],
      "metadata": {
        "id": "WJNwu6krXRgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.gen_nn_ops import Conv2D\n",
        "# CNN 모델 정의\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
        "embeddding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
        "dropout_emb = Dropout(rate=dropout_prob)(embeddding_layer)\n",
        "\n",
        "conv1 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=3,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool1 = GlobalMaxPool1D()(conv1)\n",
        "conv2 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=4,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool2 = GlobalMaxPool1D()(conv2)\n",
        "conv3 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=5,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool3 = GlobalMaxPool1D()(conv3)"
      ],
      "metadata": {
        "id": "fFbfdBSHGxl4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3, 4, 5-gram 이후 합치기\n",
        "concat = concatenate([pool1, pool2, pool3])\n",
        "\n",
        "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
        "logits = Dense(3, name='logits')(dropout_hidden)\n",
        "predictions = Dense(3, activation=tf.nn.softmax)(logits)"
      ],
      "metadata": {
        "id": "rXVU4n6BKhfe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "완전 완결 계층 구현<br>\n",
        "Dense()를 이용하여 128개의 출력 노드를 가지고, relu 활성화 함수를 사용하는 계층 생성<br>\n",
        "Dense 계층은 이전 계층에서 합성곱 연산과 맥스 풀링으로 나온 3개의 특징맵 데이터를 입력으로 받음<br>\n",
        "가장 큰 점수를 가진 노드의 위치가 XNN 모델이 예측한 class가 된다.<br>\n",
        "\n",
        "마지막 출력 노드로 정의한 logits에서 나온 점수를 소프트맥스 계층을 통해 확률 계산<br>\n",
        "클래스 분류 모델 classification problem을 학습할 때 주오 **손실값loss**를 계산하는 함수로<br>\n",
        "**sparse_categorical_crossentropy()**를 사용하고<br>\n",
        "크로스엔트로피 cross_entropy 계산을 위해 **확률값**이 필요해 **소프트맥스 계층**이 필요\n"
      ],
      "metadata": {
        "id": "kNuqUzhToaI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "1bcZJiHwNbzb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model(): 케라스 모델을 생성하는 함수<br>\n",
        "input_layer(입력 계층), predictions(출력 계층) 사용<br>\n",
        "모델 정의 후 model.compile() 함수를 통해 컴파일<br>\n",
        "**최적화**를 위하여 adam, **손실함수**에는 sparse_categorical_crossentropy<br>\n",
        "정확도 확인을 위해 metrics의 accuracy 사용"
      ],
      "metadata": {
        "id": "jWhurEzVp0Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB6muiacN2FO",
        "outputId": "0e0ec21a-35ea-4b18-b8d4-5b0d12e5a5f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "414/414 [==============================] - 39s 83ms/step - loss: 0.8818 - accuracy: 0.5790 - val_loss: 0.5129 - val_accuracy: 0.8139\n",
            "Epoch 2/5\n",
            "414/414 [==============================] - 23s 55ms/step - loss: 0.4928 - accuracy: 0.8165 - val_loss: 0.2684 - val_accuracy: 0.9234\n",
            "Epoch 3/5\n",
            "414/414 [==============================] - 24s 57ms/step - loss: 0.3056 - accuracy: 0.8996 - val_loss: 0.1615 - val_accuracy: 0.9522\n",
            "Epoch 4/5\n",
            "414/414 [==============================] - 23s 54ms/step - loss: 0.1977 - accuracy: 0.9380 - val_loss: 0.0970 - val_accuracy: 0.9712\n",
            "Epoch 5/5\n",
            "414/414 [==============================] - 17s 40ms/step - loss: 0.1349 - accuracy: 0.9584 - val_loss: 0.0670 - val_accuracy: 0.9788\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7c4288bfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫 번째 인자: 학습용 데이터셋<br>\n",
        "두 번째 인자 validation_data: 검증용 데이터셋<br>\n",
        "세 번째 인자 epochs: 학습 반복 횟수<br>\n",
        "네 번째 인자 verbose: 1인 경우 학습 시 진행 과정을 상세히 보여준다.<br>"
      ],
      "metadata": {
        "id": "3gNJzGFBrHWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가(테스트 데이터 사용)\n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jhy9SQgIOSUf",
        "outputId": "56c70b8d-886d-4fab-bdd2-e39f9c8967d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60/60 [==============================] - 0s 5ms/step - loss: 0.0722 - accuracy: 0.9772\n",
            "Accuracy: 97.715735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate() 함수를 이용하여 성능 평가<br>\n",
        "인자: 테스트용 데이터셋"
      ],
      "metadata": {
        "id": "lad2bIpKsaCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "model.save('cnn_model.h5')"
      ],
      "metadata": {
        "id": "1b-z_NqDOo9l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "PHdIQdVBs7Fd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 챗봇 문답 데이터 감정 분류 모델 사용"
      ],
      "metadata": {
        "id": "IapCoieJtiKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 모듈 임포트\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import preprocessing"
      ],
      "metadata": {
        "id": "YC-vGG5CQsJb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 읽어오기\n",
        "train_file = \"/content/ChatbotData.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()"
      ],
      "metadata": {
        "id": "Cqf1e2lLuDzR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "labels: 예측 결과와 실제 분류값 비교"
      ],
      "metadata": {
        "id": "k2UX5Bb8ZMYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "MAC_SEQ_LEN = 15\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAC_SEQ_LEN, padding='post')"
      ],
      "metadata": {
        "id": "nOnqecc5vnb4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "질문을 하나씩 꺼내 text_to_word_sequence()를 통해 단어 시퀀스로 만든 후 corpus 리스트에 저장<br>\n",
        "문장 내 모든 단어를 가지고 texts_to_sequences()를 통해 시퀀스 번호로 변환<br>\n",
        "벡터의 크기를 맞추기 위하여 pad_sequence()"
      ],
      "metadata": {
        "id": "uE8TlZhDZhZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 데이터셋 생성\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "test_ds = ds.take(2000).batch(20)"
      ],
      "metadata": {
        "id": "T5qkAsquU_PE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "패딩 처리한 시퀀스(padded_seqs) 벡터 리스트와 감정 리스트를 데이터셋 객체로 만들기<br>\n",
        "랜덤으로 섞은 후 테스트용 데이터셋을 2000개 뽑아 20개씩 배치 처리"
      ],
      "metadata": {
        "id": "vSsxsrESaXAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 감정 분류 CNN 모델 불러오기\n",
        "model = load_model('cnn_model.h5')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX1WLp1wVwPA",
        "outputId": "2318ba88-2247-4506-d9a5-71a70fc6d45d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 15)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 15, 128)      1715072     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 15, 128)      0           ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 13, 128)      49280       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 12, 128)      65664       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 11, 128)      82048       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 128)         0           ['conv1d[0][0]']                 \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 128)         0           ['conv1d_1[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Global  (None, 128)         0           ['conv1d_2[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 384)          0           ['global_max_pooling1d[0][0]',   \n",
            "                                                                  'global_max_pooling1d_1[0][0]', \n",
            "                                                                  'global_max_pooling1d_2[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          49280       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " logits (Dense)                 (None, 3)            387         ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            12          ['logits[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,961,743\n",
            "Trainable params: 1,961,743\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 불러와서 학습하고 성능 평가"
      ],
      "metadata": {
        "id": "bOM5-WjJarMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 데ㅣㅇ터셋의 10212번째 데이터 출력\n",
        "print(\"단어 시퀀스: \", corpus[10212])\n",
        "print(\"단어 인덱스 시퀀스: \", padded_seqs[10212])\n",
        "print(\"문장 분류(정답): \", labels[10212])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBPb8Ov2WP4e",
        "outputId": "7ab27d1f-364d-4f59-ee7e-c69ff0e68bd3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 시퀀스:  ['썸', '타는', '여자가', '남사친', '만나러', '간다는데', '뭐라', '해']\n",
            "단어 인덱스 시퀀스:  [   13    61   127  4320  1333 12162   856    31     0     0     0     0\n",
            "     0     0     0]\n",
            "문장 분류(정답):  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "감정 예측"
      ],
      "metadata": {
        "id": "ZGbWGCFTbea3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 데이터셋의 10212번째 데이터 감정 예측\n",
        "picks = [10212]\n",
        "predict = model.predict(padded_seqs[picks])\n",
        "predict_class = tf.math.argmax(predict, axis=1)\n",
        "print(\"감정 예측 점수: \", predict)\n",
        "print(\"감정 예측 클래스: \", predict_class.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cleIkxLRWsRd",
        "outputId": "9ca4b74c-36fa-4408-e7e7-b012b47e2d9a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "감정 예측 점수:  [[1.0250853e-05 3.0599933e-06 9.9998665e-01]]\n",
            "감정 예측 클래스:  [2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "예측 점수 반환"
      ],
      "metadata": {
        "id": "vLG3Jy3Ebl2n"
      }
    }
  ]
}